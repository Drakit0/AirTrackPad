# ğŸŒŸ *Air Trackpad* ğŸŒŸ

## ğŸ–ï¸ Introduction

In this project, we utilize *MediaPipe, **OpenCV, and a **Raspberry Pi* to create an innovative air trackpad system. The system captures hand gestures through a camera and translates them into actions such as cursor movement, clicking, scrolling, and zooming. The project combines hand detection, gesture recognition, and computer vision techniques to deliver a seamless hands-free interface.

This project was developed as the *final assignment for the Computer Vision course*, showcasing advanced real-time video processing and gesture classification.

---

## âš™ï¸ Methodology

### âœ‹ Hand Detection and Tracking

We use *MediaPipe*'s Hand Tracking solution to detect and track key hand landmarks in real-time. The system extracts 3D coordinates for critical points such as fingertips and palm centers. These landmarks form the foundation for gesture classification.

**The detection process includes:**
1. ğŸ“¸ *Frame Capture*: Real-time video is captured using OpenCV.
2. ğŸ§¹ *Preprocessing*: Frames are normalized and filtered to reduce noise.
3. ğŸ–ï¸ *Landmark Detection*: MediaPipe identifies and tracks hand landmarks.

---

### ğŸ¤š Gesture Recognition

The system recognizes complex gestures by analyzing the configuration and motion of hand landmarks. Gestures are mapped to actions such as:
- ğŸ–±ï¸ *Cursor Movement*: Index finger position defines cursor location.
- ğŸ–±ï¸ *Clicks*: Pinch gestures trigger left or right clicks.
- ğŸ“œ *Scrolling*: Vertical or horizontal two-finger swipes scroll the screen.
- ğŸ” *Zooming*: Expanding or contracting finger distances adjusts zoom levels.

**To ensure robust detection:**
- We stabilize landmarks with *Optical Flow* and *Kalman Filters* to smooth abrupt movements.
- Gestures are validated using time-based queues to avoid false positives.

---

### ğŸ“ Coordinate Mapping

Detected gestures are mapped to screen coordinates based on the display resolution. This ensures the system dynamically adapts to varying screen sizes and camera positions.

---

### ğŸ§  Advanced Techniques

1. **Image Stabilization**:
   - Motion smoothing with *Kalman Filters*.
   - Real-time adjustments for lighting and background inconsistencies.
2. **Gesture Validation**:
   - *Frechet Distance* is used to validate trajectories for complex gestures.
   - Each gesture pattern is matched to predefined models.

---

## âœ… Results

The *Air Trackpad* performs reliably under various conditions:
- ğŸ–ï¸ Gestures are accurately detected in real-time.
- ğŸš€ Cursor movements are smooth, thanks to stabilization techniques.
- ğŸ–±ï¸ Clicks, scrolling, and zoom actions are intuitive and responsive.

However, in challenging environments with excessive lighting variations or fast hand movements, minor delays or misclassifications may occur.

---

## ğŸš€ Future Improvements

To enhance the project:
1. ğŸ–±ï¸ Add support for more gestures, such as window switching or custom gestures for application shortcuts.
2. âœ‚ï¸ Implement real-time background removal to improve detection accuracy in cluttered environments.
3. âš¡ Optimize performance on Raspberry Pi by processing only a region of interest around the detected hand.

---

## ğŸ¥ Video Demo

Check out the video demonstration of the Air Trackpad:  
ğŸ‘‰ [Video Demo](#) 

---

## ğŸ‘©â€ğŸ’» Authors

This project was developed by:
- *Lydia Ruiz MartÃ­nez* ([LydiaRuizMartinez](https://github.com/LydiaRuizMartinez))  
- *Pablo TuÃ±Ã³n Laguna* ([Drakit0](https://github.com/Drakit0))

---

## ğŸ’¡ Acknowledgments

Special thanks to the course instructors for their guidance and support!

ğŸ‰ *Thank you for exploring our Air Trackpad! We hope you find it inspiring!* ğŸ‰
